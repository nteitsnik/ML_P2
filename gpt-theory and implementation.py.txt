### **Logistic Regression and Ridge Regression: Theory and Implementation**

---

### **1. Overview of the Problem**
The problem being examined here is a binary classification task to distinguish between "Fake" and "True" news articles. The dataset contains labeled news articles, where each article is marked as either fake (class 1) or true (class 0). The goal is to build a robust machine learning model that can accurately classify new, unseen articles into these categories.

To achieve this, text data is processed, cleaned, and transformed into numerical representations suitable for machine learning algorithms. Several models are implemented, including Logistic Regression, Ridge Logistic Regression, Naive Bayes, and Support Vector Machines (SVM), to evaluate their performance on this classification task.

---

### **2. Logistic Regression: Theory**
Logistic Regression is a classification algorithm used to predict the probability of a target class. It is based on the logistic function (also called the sigmoid function), which maps predictions to probabilities between 0 and 1.

The logistic function is defined as:
\[
P(Y=1|X) = \sigma(X \beta) = \frac{1}{1 + e^{-X \beta}}
\]

Where:
- \( X \): Input features (matrix of size \( n \times m \))
- \( \beta \): Coefficients (weights) to be learned
- \( \sigma \): Sigmoid function

**Objective**:
- Minimize the binary cross-entropy loss:
\[
L(\beta) = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]
\]

---

### **3. Ridge Regression: Theory**
Ridge Regression is a form of regularized linear regression that includes a penalty term to prevent overfitting by shrinking coefficients. It is commonly used for regression problems, but the concept can be extended to Logistic Regression, where it is called **Ridge Logistic Regression**.

The Ridge penalty adds an \( L_2 \)-regularization term to the loss function:
\[
L(\beta) = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right] + \frac{\lambda}{2} \|\beta\|_2^2
\]

Where:
- \( \lambda \): Regularization parameter controlling the strength of the penalty
- \( \|\beta\|_2^2 \): Sum of the squared coefficients (\( L_2 \)-norm)

Regularization helps prevent overfitting by penalizing large coefficients, thereby encouraging simpler models.

---

### **4. Implementation: Logistic and Ridge Logistic Regression**

#### **Step 1: Import Required Libraries**
```python
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn import svm
```

#### **Step 2: Data Preprocessing**
```python
# Text Cleaning
textdata['clean_text'] = textdata['tokens'].apply(lambda tokens: ' '.join(tokens))

# Vectorization
vectorizers = [CountVectorizer(), TfidfVectorizer()]
Y = textdata['Class']
```

#### **Step 3: Logistic Regression Implementation**
```python
for vectorizer in vectorizers:
    X = vectorizer.fit_transform(textdata['clean_text'])
    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)

    # Logistic Regression
    logistic_model = LogisticRegression()
    logistic_model.fit(X_train, Y_train)

    # Predictions
    logistic_Y_pred = logistic_model.predict(X_test)

    # Evaluation
    logistic_accuracy = accuracy_score(Y_test, logistic_Y_pred)
    print(f"Logistic Regression Accuracy with {vectorizer.__class__.__name__}: {logistic_accuracy * 100:.2f}%")
    print("Confusion Matrix:\n", confusion_matrix(Y_test, logistic_Y_pred))
    print("Classification Report:\n", classification_report(Y_test, logistic_Y_pred))
```

#### **Step 4: Ridge Logistic Regression Implementation**
```python
    # Ridge Logistic Regression
    ridge_model = LogisticRegression(penalty='l2', C=0.1)
    ridge_model.fit(X_train, Y_train)

    # Predictions
    ridge_Y_pred = ridge_model.predict(X_test)

    # Evaluation
    ridge_accuracy = accuracy_score(Y_test, ridge_Y_pred)
    print(f"Ridge Logistic Regression Accuracy with {vectorizer.__class__.__name__}: {ridge_accuracy * 100:.2f}%")
    print("Confusion Matrix:\n", confusion_matrix(Y_test, ridge_Y_pred))
    print("Classification Report:\n", classification_report(Y_test, ridge_Y_pred))
```

#### **Step 5: Naive Bayes Implementation**
```python
    # Naive Bayes
    nb_model = MultinomialNB()
    nb_model.fit(X_train, Y_train)

    # Predictions
    nb_Y_pred = nb_model.predict(X_test)

    # Evaluation
    nb_accuracy = accuracy_score(Y_test, nb_Y_pred)
    print(f"Naive Bayes Accuracy with {vectorizer.__class__.__name__}: {nb_accuracy * 100:.2f}%")
    print("Confusion Matrix:\n", confusion_matrix(Y_test, nb_Y_pred))
    print("Classification Report:\n", classification_report(Y_test, nb_Y_pred))
```

#### **Step 6: Support Vector Machines (SVM) Implementation**
```python
    # SVM
    svm_model = svm.SVC(kernel='linear')
    svm_model.fit(X_train, Y_train)

    # Predictions
    svm_Y_pred = svm_model.predict(X_test)

    # Evaluation
    svm_accuracy = accuracy_score(Y_test, svm_Y_pred)
    print(f"SVM Accuracy with {vectorizer.__class__.__name__}: {svm_accuracy * 100:.2f}%")
    print("Confusion Matrix:\n", confusion_matrix(Y_test, svm_Y_pred))
    print("Classification Report:\n", classification_report(Y_test, svm_Y_pred))
```

#### **Step 7: Hyperparameter Tuning for Ridge Logistic Regression**
```python
    # Define parameter grid
    param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}

    # Grid search with Logistic Regression
    grid = GridSearchCV(LogisticRegression(penalty='l2', solver='liblinear'), param_grid, cv=5, scoring='accuracy')
    grid.fit(X_train, Y_train)

    # Best model and parameter
    print("Best C (regularization):", grid.best_params_)
    best_ridge_model = grid.best_estimator_

    # Evaluate on test set
    grid_Y_pred = best_ridge_model.predict(X_test)
    grid_accuracy = accuracy_score(Y_test, grid_Y_pred)
    print(f"Grid Search Ridge Accuracy with {vectorizer.__class__.__name__}: {grid_accuracy * 100:.2f}%")
```

---

### **5. Final Script with All Methods**
Below is the consolidated script combining all methods:

```python
# Preprocessing
textdata['clean_text'] = textdata['tokens'].apply(lambda tokens: ' '.join(tokens))
vectorizers = [CountVectorizer(), TfidfVectorizer()]
Y = textdata['Class']

# Iterate through vectorizers
for vectorizer in vectorizers:
    X = vectorizer.fit_transform(textdata['clean_text'])
    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)

    # Logistic Regression
    logistic_model = LogisticRegression()
    logistic_model.fit(X_train, Y_train)
    logistic_Y_pred = logistic_model.predict(X_test)
    print(f"Logistic Regression Accuracy with {vectorizer.__class__.__name__}: {accuracy_score(Y_test, logistic_Y_pred) * 100:.2f}%")

    # Ridge Logistic Regression
    ridge_model = LogisticRegression(penalty='l2', C=0.1)
    ridge_model.fit(X_train, Y_train)
    ridge_Y_pred = ridge_model.predict(X_test)
    print(f"Ridge Logistic Regression Accuracy with {vectorizer.__class

from sklearn.tree import DecisionTreeClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

# Assume 'textdata' DataFrame is already prepared with 'clean_text' and 'Class'

# Vectorization
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(textdata['clean_text'])
Y = textdata['Class']

# Split Dataset
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Initialize and train Decision Tree Classifier
dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train, Y_train)

# Predictions
Y_pred = dt_classifier.predict(X_test)

# Evaluation
accuracy = accuracy_score(Y_test, Y_pred)
print(f"Decision Tree Accuracy: {accuracy * 100:.2f}%")
print("Classification Report:\n", classification_report(Y_test, Y_pred))
